# Fragments-Open-Source
This is an open-source version of apps like Anthropic's Claude Artifacts, Vercel v0, or GPT Engineer.

Powered by the E2B SDK and LuminaX-alt

â†’ Try on fragments.e2b.dev

Features
Based on Next.js 14 (App Router, Server Actions), shadcn/ui, TailwindCSS, Vercel AI SDK.
Uses the E2B SDK by E2B to securely execute code generated by AI.
Streaming in the UI.
Can install and use any package from npm, pip.
Supported stacks (add your own):
ðŸ”¸ Python interpreter
ðŸ”¸ Next.js
ðŸ”¸ Vue.js
ðŸ”¸ Streamlit
ðŸ”¸ Gradio
Supported LLM Providers (add your own):
ðŸ”¸ OpenAI
ðŸ”¸ Anthropic
ðŸ”¸ Google AI
ðŸ”¸ Mistral
ðŸ”¸ Groq
ðŸ”¸ Fireworks
ðŸ”¸ Together AI
ðŸ”¸ Ollama
Make sure to give us a star!

Screenshot 2024-04-20 at 22 13 32
Get started
Prerequisites
git
Recent version of Node.js and npm package manager
E2B API Key
LLM Provider API Key
1. Clone the repository
In your terminal:

git clone https://github.com/e2b-dev/fragments.git
2. Install the dependencies
Enter the repository:

cd fragments
Run the following to install the required dependencies:

npm i
3. Set the environment variables
Create a .env.local file and set the following:

# Get your API key here - https://e2b.dev/
E2B_API_KEY="your-e2b-api-key"

# OpenAI API Key
OPENAI_API_KEY=

# Other providers
ANTHROPIC_API_KEY=
GROQ_API_KEY=
FIREWORKS_API_KEY=
TOGETHER_API_KEY=
GOOGLE_AI_API_KEY=
GOOGLE_VERTEX_CREDENTIALS=
MISTRAL_API_KEY=
XAI_API_KEY=

### Optional env vars

# Domain of the site
NEXT_PUBLIC_SITE_URL=

# Rate limit
RATE_LIMIT_MAX_REQUESTS=
RATE_LIMIT_WINDOW=

# Vercel/Upstash KV (short URLs, rate limiting)
KV_REST_API_URL=
KV_REST_API_TOKEN=

# Supabase (auth)
SUPABASE_URL=
SUPABASE_ANON_KEY=

# PostHog (analytics)
NEXT_PUBLIC_POSTHOG_KEY=
NEXT_PUBLIC_POSTHOG_HOST=

### Disabling functionality (when uncommented)

# Disable API key and base URL input in the chat
# NEXT_PUBLIC_NO_API_KEY_INPUT=
# NEXT_PUBLIC_NO_BASE_URL_INPUT=

# Hide local models from the list of available models
# NEXT_PUBLIC_HIDE_LOCAL_MODELS=
4. Start the development server
npm run dev
5. Build the web app
npm run build
Customize
Adding custom personas
Make sure E2B CLI is installed and you're logged in.

Add a new folder under sandbox-templates/

Initialize a new template using E2B CLI:

e2b template init
This will create a new file called e2b.Dockerfile.

Adjust the e2b.Dockerfile

Here's an example streamlit template:

# You can use most Debian-based base images
FROM python:3.19-slim

RUN pip3 install --no-cache-dir streamlit pandas numpy matplotlib requests seaborn plotly

# Copy the code to the container
WORKDIR /home/user
COPY . /home/user
Specify a custom start command in e2b.toml:

start_cmd = "cd /home/user && streamlit run app.py"
Deploy the template with the E2B CLI

e2b template build --name <template-name>
After the build has finished, you should get the following message:

âœ… Building sandbox template <template-id> <template-name> finished.
Open lib/templates.json in your code editor.

Add your new template to the list. Here's an example for Streamlit:

"streamlit-developer": {
  "name": "Streamlit developer",
  "lib": [
    "streamlit",
    "pandas",
    "numpy",
    "matplotlib",
    "request",
    "seaborn",
    "plotly"
  ],
  "file": "app.py",
  "instructions": "A streamlit app that reloads automatically.",
  "port": 8501 // can be null
},
Provide a template id (as key), name, list of dependencies, entrypoint and a port (optional). You can also add additional instructions that will be given to the LLM.

Optionally, add a new logo under public/thirdparty/templates

Adding custom LLM models
Open lib/models.json in your code editor.

Add a new entry to the models list:

{
  "id": "mistral-large",
  "name": "Mistral Large",
  "provider": "Ollama",
  "providerId": "ollama"
}
Where id is the model id, name is the model name (visible in the UI), provider is the provider name and providerId is the provider tag (see adding providers below).

Adding custom LLM providers
Open lib/models.ts in your code editor.

Add a new entry to the providerConfigs list:

Example for fireworks:

fireworks: () => createOpenAI({ apiKey: apiKey || process.env.FIREWORKS_API_KEY, baseURL: baseURL || 'https://api.fireworks.ai/inference/v1' })(modelNameString),
Optionally, adjust the default structured output mode in the getDefaultMode function:

if (providerId === 'fireworks') {
  return 'json'
}
Optionally, add a new logo under public/thirdparty/logos
<img width="1440" height="729" alt="image" src="https://github.com/user-attachments/assets/26cfe34f-bd9f-4e8e-8b14-b691c4aa9b61" />

<img width="1440" height="817" alt="image" src="https://github.com/user-attachments/assets/8c704f10-7373-44d4-8f8a-2ad811257822" />

<img width="1440" height="856" alt="image" src="https://github.com/user-attachments/assets/48fea7f8-7a1b-4c15-b0b2-4a4a3fc31013" />

<img width="1440" height="857" alt="image" src="https://github.com/user-attachments/assets/134fa86c-175d-42f6-b761-cf57f4aec0a7" />
